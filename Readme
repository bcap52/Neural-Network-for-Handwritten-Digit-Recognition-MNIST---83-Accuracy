# Handwritten Digit Recognition using MNIST - 83% Accuracy

A two-layer neural network built from scratch that achieves 83% accuracy on handwritten digit recognition using the MNIST dataset. The implementation includes forward propagation, backpropagation, and gradient descent optimization.

## Overview

This project implements a neural network from scratch using only NumPy, without any deep learning frameworks. The network successfully classifies handwritten digits with 83% accuracy on the MNIST dataset.

## Dataset

MNIST dataset containing:
- 60,000 training images
- 10,000 test images  
- 28x28 pixel grayscale images
- 10 classes (digits 0-9)

## Architecture

```
Input Layer (784 neurons) -> Hidden Layer (10 neurons, ReLU) -> Output Layer (10 neurons, Softmax)
```

Components:
- Activation Functions: ReLU for hidden layer, Softmax for output
- Loss Function: Cross-entropy loss
- Optimization: Gradient descent with backpropagation

## Installation

```bash
pip install numpy pandas matplotlib
```

## Usage

Basic training:
```python
# Load and preprocess data
data = pd.read_csv('train.csv')

# Train the model
W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)

# Make predictions
predictions = make_predictions(X_test, W1, b1, W2, b2)
```

## Results

- Training Accuracy: 83%
- Training Iterations: 500
- Learning Rate: 0.10

## Key Functions

- init_parameters(): Initialize weights and biases
- forward_propogation(): Forward pass through network
- back_propogation(): Calculate gradients via backpropagation
- gradient_descent(): Main training loop
- make_predictions(): Generate predictions for new data

## Mathematical Implementation

Forward Propagation:
```
Z1 = W1 * X + b1
A1 = ReLU(Z1)
Z2 = W2 * A1 + b2  
A2 = Softmax(Z2)
```

Backpropagation:
```
dZ2 = A2 - Y (one-hot encoded)
dW2 = (1/m) * dZ2 * A1^T
dZ1 = W2^T * dZ2 * ReLU'(Z1)
dW1 = (1/m) * dZ1 * X^T
```

## File Structure

```
mnist-neural-network/
├── train.csv
├── neural_network.py
├── README.md
└── requirements.txt
```

## Potential Improvements

- Better weight initialization (Xavier/He)
- Learning rate decay
- Increased hidden layer size
- Regularization techniques
- Additional layers for deeper network

